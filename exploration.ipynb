{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from utils import load_model\n",
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "import os\n",
    "\n",
    "PATH_TO_EVAL = 'evaluation/'\n",
    "EVAL_IMAGES = os.listdir(PATH_TO_EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'models/resnet18_weights_best_acc.tar'  # path to the downloaded pre-trained model\n",
    "use_gpu = False  # set to True if you want to load weights on the GPU\n",
    "model = resnet18(num_classes=1081)  # 1081 classes in Pl@ntNet-300K\n",
    "\n",
    "load_model(model, filename=filename, use_gpu=use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.models.resnet.ResNet"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight                               torch.Size([64, 3, 7, 7])      True\n",
      "bn1.weight                                 torch.Size([64])               True\n",
      "bn1.bias                                   torch.Size([64])               True\n",
      "layer1.0.conv1.weight                      torch.Size([64, 64, 3, 3])     True\n",
      "layer1.0.bn1.weight                        torch.Size([64])               True\n",
      "layer1.0.bn1.bias                          torch.Size([64])               True\n",
      "layer1.0.conv2.weight                      torch.Size([64, 64, 3, 3])     True\n",
      "layer1.0.bn2.weight                        torch.Size([64])               True\n",
      "layer1.0.bn2.bias                          torch.Size([64])               True\n",
      "layer1.1.conv1.weight                      torch.Size([64, 64, 3, 3])     True\n",
      "layer1.1.bn1.weight                        torch.Size([64])               True\n",
      "layer1.1.bn1.bias                          torch.Size([64])               True\n",
      "layer1.1.conv2.weight                      torch.Size([64, 64, 3, 3])     True\n",
      "layer1.1.bn2.weight                        torch.Size([64])               True\n",
      "layer1.1.bn2.bias                          torch.Size([64])               True\n",
      "layer2.0.conv1.weight                      torch.Size([128, 64, 3, 3])    True\n",
      "layer2.0.bn1.weight                        torch.Size([128])              True\n",
      "layer2.0.bn1.bias                          torch.Size([128])              True\n",
      "layer2.0.conv2.weight                      torch.Size([128, 128, 3, 3])   True\n",
      "layer2.0.bn2.weight                        torch.Size([128])              True\n",
      "layer2.0.bn2.bias                          torch.Size([128])              True\n",
      "layer2.0.downsample.0.weight               torch.Size([128, 64, 1, 1])    True\n",
      "layer2.0.downsample.1.weight               torch.Size([128])              True\n",
      "layer2.0.downsample.1.bias                 torch.Size([128])              True\n",
      "layer2.1.conv1.weight                      torch.Size([128, 128, 3, 3])   True\n",
      "layer2.1.bn1.weight                        torch.Size([128])              True\n",
      "layer2.1.bn1.bias                          torch.Size([128])              True\n",
      "layer2.1.conv2.weight                      torch.Size([128, 128, 3, 3])   True\n",
      "layer2.1.bn2.weight                        torch.Size([128])              True\n",
      "layer2.1.bn2.bias                          torch.Size([128])              True\n",
      "layer3.0.conv1.weight                      torch.Size([256, 128, 3, 3])   True\n",
      "layer3.0.bn1.weight                        torch.Size([256])              True\n",
      "layer3.0.bn1.bias                          torch.Size([256])              True\n",
      "layer3.0.conv2.weight                      torch.Size([256, 256, 3, 3])   True\n",
      "layer3.0.bn2.weight                        torch.Size([256])              True\n",
      "layer3.0.bn2.bias                          torch.Size([256])              True\n",
      "layer3.0.downsample.0.weight               torch.Size([256, 128, 1, 1])   True\n",
      "layer3.0.downsample.1.weight               torch.Size([256])              True\n",
      "layer3.0.downsample.1.bias                 torch.Size([256])              True\n",
      "layer3.1.conv1.weight                      torch.Size([256, 256, 3, 3])   True\n",
      "layer3.1.bn1.weight                        torch.Size([256])              True\n",
      "layer3.1.bn1.bias                          torch.Size([256])              True\n",
      "layer3.1.conv2.weight                      torch.Size([256, 256, 3, 3])   True\n",
      "layer3.1.bn2.weight                        torch.Size([256])              True\n",
      "layer3.1.bn2.bias                          torch.Size([256])              True\n",
      "layer4.0.conv1.weight                      torch.Size([512, 256, 3, 3])   True\n",
      "layer4.0.bn1.weight                        torch.Size([512])              True\n",
      "layer4.0.bn1.bias                          torch.Size([512])              True\n",
      "layer4.0.conv2.weight                      torch.Size([512, 512, 3, 3])   True\n",
      "layer4.0.bn2.weight                        torch.Size([512])              True\n",
      "layer4.0.bn2.bias                          torch.Size([512])              True\n",
      "layer4.0.downsample.0.weight               torch.Size([512, 256, 1, 1])   True\n",
      "layer4.0.downsample.1.weight               torch.Size([512])              True\n",
      "layer4.0.downsample.1.bias                 torch.Size([512])              True\n",
      "layer4.1.conv1.weight                      torch.Size([512, 512, 3, 3])   True\n",
      "layer4.1.bn1.weight                        torch.Size([512])              True\n",
      "layer4.1.bn1.bias                          torch.Size([512])              True\n",
      "layer4.1.conv2.weight                      torch.Size([512, 512, 3, 3])   True\n",
      "layer4.1.bn2.weight                        torch.Size([512])              True\n",
      "layer4.1.bn2.bias                          torch.Size([512])              True\n",
      "fc.weight                                  torch.Size([1081, 512])        True\n",
      "fc.bias                                    torch.Size([1081])             True\n"
     ]
    }
   ],
   "source": [
    "# get shape of the model\n",
    "for name, param in model.named_parameters():\n",
    "    print(f'{name:42} {str(param.shape):30} {param.requires_grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load monstera image\n",
    "monstera = Image.open('monstera.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )])\n",
    "monstera_tensor = transform(monstera).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model(monstera_tensor)\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "# Get the predicted class and the confidence score\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "confidence_score = probabilities[predicted_class].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([11.6517]), tensor([285]), 0.5636008977890015)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, predicted_class, confidence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: annie-spratt-8mqOw4DBBSg-unsplash.jpg    | Predicted class: tensor([334])   | Confidence score: 0.91\n",
      "Image: annie-spratt-hX_hf2lPpUU-unsplash.jpg    | Predicted class: tensor([322])   | Confidence score: 0.41\n",
      "Image: chris-lee-70l1tDAI6rM-unsplash.jpg       | Predicted class: tensor([858])   | Confidence score: 0.14\n",
      "Image: feey-7d7OR-RvufU-unsplash.jpg            | Predicted class: tensor([702])   | Confidence score: 0.71\n",
      "Image: igor-son-FV_PxCqgtwc-unsplash.jpg        | Predicted class: tensor([92])    | Confidence score: 0.30\n",
      "Image: lauren-mancke-DpphPG9ENsI-unsplash.jpg   | Predicted class: tensor([434])   | Confidence score: 0.37\n",
      "Image: linh-le-Ebwp2-6BG8E-unsplash.jpg         | Predicted class: tensor([322])   | Confidence score: 0.46\n",
      "Image: monstera.jpg                             | Predicted class: tensor([285])   | Confidence score: 0.56\n",
      "Image: ren-ran-bBiuSdck8tU-unsplash.jpg         | Predicted class: tensor([602])   | Confidence score: 0.19\n",
      "Image: teemu-paananen-OOE4xAnBhKo-unsplash.jpg  | Predicted class: tensor([28])    | Confidence score: 0.79\n"
     ]
    }
   ],
   "source": [
    "def predict(path_to_image, transform, model):\n",
    "    image = Image.open(path_to_image)\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "    # Get the predicted class and the confidence score\n",
    "    _, predicted_class = torch.max(output, 1)\n",
    "    confidence_score = probabilities[predicted_class].item()\n",
    "\n",
    "    return _, predicted_class, confidence_score\n",
    "\n",
    "# predict all images in the evaluation folder\n",
    "for image in EVAL_IMAGES:\n",
    "    path_to_image = PATH_TO_EVAL + image\n",
    "    _, predicted_class, confidence_score = predict(path_to_image, transform, model)\n",
    "    print(f'Image: {image:40} | Predicted class: {str(predicted_class):15} | Confidence score: {confidence_score:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# load plantnet300k_metadata.json\n",
    "import json\n",
    "with open('plantnet300k_metadata.json') as f:\n",
    "    metadata = json.load(f)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a80a1fb1383acdec6f9073553c2e0645ea2d55da {'species_id': '1358689', 'obs_id': '1004630654', 'organ': 'fruit', 'is_shareable': True, 'v1_id': '', 'author': 'Kieron Dunne', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "eae259d1df04404150ba79219bf84a1597cb3254 {'species_id': '1385937', 'obs_id': '1007897628', 'organ': 'leaf', 'is_shareable': True, 'v1_id': '', 'author': 'Petrika Mirna', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "511b857913c00ef1fee795077de154f077e4c1a9 {'species_id': '1359616', 'obs_id': '1005326602', 'organ': 'fruit', 'is_shareable': True, 'v1_id': '', 'author': 'gerard dumont', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "01e217a0cfa6620d27e4fc3c43ce73949ab37287 {'species_id': '1359517', 'obs_id': '1008500291', 'organ': 'flower', 'is_shareable': True, 'v1_id': '', 'author': 'Alfredo', 'license': 'cc-by-sa', 'split': 'val'}\n",
      "04fc5ebbc933427550a71196cb49410fe8becb55 {'species_id': '1384497', 'obs_id': '1004762218', 'organ': 'fruit', 'is_shareable': True, 'v1_id': '', 'author': \"that's someguy\", 'license': 'cc-by-sa', 'split': 'train'}\n",
      "f15824f8e5ea894f7c2b2a58fea92c62936b14d4 {'species_id': '1363227', 'obs_id': '1004784824', 'organ': 'flower', 'is_shareable': True, 'v1_id': '', 'author': 'Thaddeus Tomich', 'license': 'cc-by-sa', 'split': 'test'}\n",
      "444d1df25b53adb3dbc079c25419f7a99eb99fd7 {'species_id': '1394591', 'obs_id': '1008610853', 'organ': 'flower', 'is_shareable': True, 'v1_id': '', 'author': 'Isabelle de caunes', 'license': 'cc-by-sa', 'split': 'val'}\n",
      "3eebf2063f0d31c81b34203052e21f01bcd97e71 {'species_id': '1359486', 'obs_id': '1004331162', 'organ': 'flower', 'is_shareable': True, 'v1_id': 'fa9c114abb53803a14ddd2275073128e/01b5f5a0-de12-4352-a0ba-287a4c29ebe4.jpg', 'author': 'Iulia Demeter', 'license': 'cc-by-sa', 'split': 'val'}\n",
      "a9f36e9397baefbdbaee76fb3b2eb733c6386cb3 {'species_id': '1360835', 'obs_id': '1006970989', 'organ': 'leaf', 'is_shareable': True, 'v1_id': '', 'author': 'Stéphane Mars', 'license': 'cc-by-sa', 'split': 'test'}\n",
      "577953cdbcc1d89751b035fbf5f5f8f223175b46 {'species_id': '1364164', 'obs_id': '1006614662', 'organ': 'leaf', 'is_shareable': True, 'v1_id': '', 'author': 'Jana Pánková', 'license': 'cc-by-sa', 'split': 'test'}\n",
      "d9a5f4f01041eccae33ec707daf4bc64c99615cc {'species_id': '1391250', 'obs_id': '1008000908', 'organ': 'leaf', 'is_shareable': True, 'v1_id': '', 'author': 'Luc Guislain', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "f86f6e20f2fb1daed99ccd13c9af0150f59c47e1 {'species_id': '1362954', 'obs_id': '1005086091', 'organ': 'leaf', 'is_shareable': True, 'v1_id': '', 'author': 'Clémence MICHEL', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "50b8823fac26d4a6e0cd7cc6b0335a4dc279b10f {'species_id': '1363227', 'obs_id': '1008103190', 'organ': 'flower', 'is_shareable': True, 'v1_id': '', 'author': 'Benoît Sevenier', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "b0f3378d7aad2501ed21f71b08d3f2e1f5325cee {'species_id': '1357330', 'obs_id': '1006538959', 'organ': 'flower', 'is_shareable': True, 'v1_id': '', 'author': 'laureen Noël', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "340ee88fface8ad39e46d84b1f779cce2d4ad8e2 {'species_id': '1359620', 'obs_id': '1003665144', 'organ': 'leaf', 'is_shareable': True, 'v1_id': '', 'author': 'Djamel Grid', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "3fbcdd61737e830fc8dfb9a71df2e75ceef8ae23 {'species_id': '1359616', 'obs_id': '1006520105', 'organ': 'flower', 'is_shareable': True, 'v1_id': '', 'author': 'julien fort', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "a3bae6b114b647abf19d8010585f1f4e4800fd6c {'species_id': '1359517', 'obs_id': '1008384993', 'organ': 'flower', 'is_shareable': True, 'v1_id': '', 'author': 'Hanna Cichocka', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "bec250057494eefdcbee238b92dcf13bfb7c877b {'species_id': '1389921', 'obs_id': '1008830263', 'organ': 'flower', 'is_shareable': True, 'v1_id': '', 'author': 'Laura Sanders', 'license': 'cc-by-sa', 'split': 'test'}\n",
      "00771a6e1d68fb230ca6e41acfeda86c26469e09 {'species_id': '1358766', 'obs_id': '1003364625', 'organ': 'flower', 'is_shareable': True, 'v1_id': '97189394b9b29183fd04af6c947a1cce/a356075f-bfa3-4083-b248-15938b743d92.jpg', 'author': 'Victoria Rivera', 'license': 'cc-by-sa', 'split': 'train'}\n",
      "6b672da90cc93c0d6ab2830d2d712f8947548d3d {'species_id': '1394460', 'obs_id': '1006337180', 'organ': 'leaf', 'is_shareable': True, 'v1_id': '', 'author': 'Lucie Zizlavska', 'license': 'cc-by-sa', 'split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "# print 20 random keys and their values\n",
    "import random\n",
    "random.seed(42)\n",
    "random_keys = random.sample(list(metadata.keys()), 20)\n",
    "for key in random_keys:\n",
    "    print(f'{key:10} {metadata[key]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all unique obs_id in metadata\n",
    "species_ids = set()\n",
    "for key in metadata.keys():\n",
    "    species_ids.add(metadata[key]['species_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1081"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(species_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1355868',\n",
       " '1355920',\n",
       " '1355932',\n",
       " '1355936',\n",
       " '1355937',\n",
       " '1355955',\n",
       " '1355959',\n",
       " '1355961',\n",
       " '1355978',\n",
       " '1355990']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort obs_ids and print first 10\n",
    "species_ids = sorted(species_ids)\n",
    "species_ids[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
